{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\botorch\\models\\gpytorch.py:129: UserWarning: The model inputs are of type torch.float32. It is strongly recommended to use double precision in BoTorch, as this improves both precision and stability and can help avoid numerical errors. See https://github.com/pytorch/botorch/discussions/1444\n",
      "  warnings.warn(_get_single_precision_warning(X.dtype), UserWarning)\n",
      "c:\\Users\\springnuance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\botorch\\models\\utils\\assorted.py:201: InputDataWarning: Input data is not standardized. Please consider scaling the input to zero mean and unit variance.\n",
      "  warnings.warn(msg, InputDataWarning)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "qExpectedHypervolumeImprovement.__init__() missing 1 required positional argument: 'partitioning'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m reference_point \u001b[38;5;241m=\u001b[39m infer_reference_point(train_y)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Define the acquisition function\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m qEHVI \u001b[38;5;241m=\u001b[39m \u001b[43mqExpectedHypervolumeImprovement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mref_point\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreference_point\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# reference point for hypervolume calculation\u001b[39;49;00m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_simplex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# sampler for the qEHVI acquisition function\u001b[39;49;00m\n\u001b[0;32m     36\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Optimize the acquisition function\u001b[39;00m\n\u001b[0;32m     39\u001b[0m new_candidate, _ \u001b[38;5;241m=\u001b[39m optimize_acqf(\n\u001b[0;32m     40\u001b[0m     acq_function\u001b[38;5;241m=\u001b[39mqEHVI,\n\u001b[0;32m     41\u001b[0m     bounds\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m], [\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m]]),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m     raw_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m     45\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: qExpectedHypervolumeImprovement.__init__() missing 1 required positional argument: 'partitioning'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from botorch.models import SingleTaskGP\n",
    "from botorch.fit import fit_gpytorch_model\n",
    "from gpytorch.mlls.exact_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.acquisition.multi_objective import qExpectedHypervolumeImprovement\n",
    "from botorch.utils.multi_objective import infer_reference_point\n",
    "from botorch.utils.sampling import sample_simplex\n",
    "\n",
    "# Define your objective functions\n",
    "def objective_function(x):\n",
    "    # This is where you define your multi-objective function\n",
    "    # For example, two objectives based on input tensor x\n",
    "    objective1 = -torch.norm(x - 0.5, dim=-1)\n",
    "    objective2 = torch.sum(x, dim=-1)\n",
    "    return torch.stack([-objective1, objective2], dim=-1)\n",
    "\n",
    "# Generate some random data\n",
    "# For example, 10 data points in 2D space\n",
    "train_x = torch.rand(10, 2)\n",
    "train_y = objective_function(train_x)\n",
    "\n",
    "# Fit a GP model\n",
    "model = SingleTaskGP(train_x, train_y)\n",
    "mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "fit_gpytorch_model(mll)\n",
    "\n",
    "# Infer the reference point\n",
    "reference_point = infer_reference_point(train_y)\n",
    "\n",
    "# Define the acquisition function\n",
    "qEHVI = qExpectedHypervolumeImprovement(\n",
    "    model=model,\n",
    "    ref_point=reference_point.tolist(),  # reference point for hypervolume calculation\n",
    "    sampler=sample_simplex,  # sampler for the qEHVI acquisition function\n",
    "    partitioning=N\n",
    ")\n",
    "\n",
    "# Optimize the acquisition function\n",
    "new_candidate, _ = optimize_acqf(\n",
    "    acq_function=qEHVI,\n",
    "    bounds=torch.tensor([[0.0, 0.0], [1.0, 1.0]]),\n",
    "    q=1,\n",
    "    num_restarts=5,\n",
    "    raw_samples=20,\n",
    ")\n",
    "\n",
    "print(new_candidate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
